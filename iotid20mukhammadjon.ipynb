{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mukhammadjontursunaliev/Project/blob/main/iotid20mukhammadjon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IoTID20 Data Cleaning and Preprocessing\n",
        "\n",
        "This notebook guides you through cleaning and preprocessing the IoTID20 dataset\n",
        "for anomaly detection model training. As a Data Engineer/Scientist, we'll:\n",
        "\n",
        "1. Mount Google Drive  \n",
        "2. Load the dataset  \n",
        "3. Inspect data structure and quality  \n",
        "4. Clean missing or invalid records  \n",
        "5. Engineer features and split data  \n",
        "6. Scale features and save artifacts  \n",
        "\n"
      ],
      "metadata": {
        "id": "S9K6yfv22IIE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbluNY5v2CJ-",
        "outputId": "623a7f03-66bf-44e2-ba9a-d95a8454e179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# 1. Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Imports and Configuration\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import joblib\n",
        "import os\n"
      ],
      "metadata": {
        "id": "C-ehaZB42LZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Load IoTID20 Dataset\n",
        "data_path = '/content/drive/MyDrive/Datasets/IoT Network Intrusion Dataset.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "print(f'Dataset loaded with shape: {df.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvOFdbDh4xMT",
        "outputId": "800e3d5d-ba08-4835-a990-782ac6081b7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded with shape: (625783, 86)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Inspection\n"
      ],
      "metadata": {
        "id": "r12xKjn747Qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect basic structure and missing values\n",
        "print(df.dtypes)\n",
        "print(df.head())\n",
        "print('Missing values per column:')\n",
        "print(df.isnull().sum())\n",
        "print('Duplicate rows:', df.duplicated().sum())\n",
        "print('Label distribution:')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVTlmbjq43Qp",
        "outputId": "25f859e0-ba2a-4a42-8d6b-76b2c2fe1bdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flow_ID      object\n",
            "Src_IP       object\n",
            "Src_Port      int64\n",
            "Dst_IP       object\n",
            "Dst_Port      int64\n",
            "             ...   \n",
            "Idle_Max    float64\n",
            "Idle_Min    float64\n",
            "Label        object\n",
            "Cat          object\n",
            "Sub_Cat      object\n",
            "Length: 86, dtype: object\n",
            "                                     Flow_ID           Src_IP  Src_Port  \\\n",
            "0   192.168.0.13-192.168.0.16-10000-10101-17     192.168.0.13     10000   \n",
            "1    192.168.0.13-222.160.179.132-554-2179-6  222.160.179.132      2179   \n",
            "2     192.168.0.13-192.168.0.16-9020-52727-6     192.168.0.16     52727   \n",
            "3     192.168.0.13-192.168.0.16-9020-52964-6     192.168.0.16     52964   \n",
            "4  192.168.0.1-239.255.255.250-36763-1900-17      192.168.0.1     36763   \n",
            "\n",
            "            Dst_IP  Dst_Port  Protocol               Timestamp  Flow_Duration  \\\n",
            "0     192.168.0.16     10101        17  25/07/2019 03:25:53 AM             75   \n",
            "1     192.168.0.13       554         6  26/05/2019 10:11:06 PM           5310   \n",
            "2     192.168.0.13      9020         6  11/07/2019 01:24:48 AM            141   \n",
            "3     192.168.0.13      9020         6  04/09/2019 03:58:17 AM            151   \n",
            "4  239.255.255.250      1900        17  10/09/2019 01:41:18 AM            153   \n",
            "\n",
            "   Tot_Fwd_Pkts  Tot_Bwd_Pkts  ...  Active_Std  Active_Max  Active_Min  \\\n",
            "0             1             1  ...         0.0         0.0         0.0   \n",
            "1             1             2  ...         0.0         0.0         0.0   \n",
            "2             0             3  ...         0.0         0.0         0.0   \n",
            "3             0             2  ...         0.0         0.0         0.0   \n",
            "4             2             1  ...         0.0         0.0         0.0   \n",
            "\n",
            "   Idle_Mean     Idle_Std  Idle_Max  Idle_Min    Label    Cat  \\\n",
            "0       75.0     0.000000      75.0      75.0  Anomaly  Mirai   \n",
            "1     2655.0  2261.327486    4254.0    1056.0  Anomaly    DoS   \n",
            "2       70.5     0.707107      71.0      70.0  Anomaly   Scan   \n",
            "3      151.0     0.000000     151.0     151.0  Anomaly  Mirai   \n",
            "4       76.5     0.707107      77.0      76.0  Anomaly  Mirai   \n",
            "\n",
            "                 Sub_Cat  \n",
            "0      Mirai-Ackflooding  \n",
            "1        DoS-Synflooding  \n",
            "2           Scan Port OS  \n",
            "3  Mirai-Hostbruteforceg  \n",
            "4  Mirai-Hostbruteforceg  \n",
            "\n",
            "[5 rows x 86 columns]\n",
            "Missing values per column:\n",
            "Flow_ID     0\n",
            "Src_IP      0\n",
            "Src_Port    0\n",
            "Dst_IP      0\n",
            "Dst_Port    0\n",
            "           ..\n",
            "Idle_Max    0\n",
            "Idle_Min    0\n",
            "Label       0\n",
            "Cat         0\n",
            "Sub_Cat     0\n",
            "Length: 86, dtype: int64\n",
            "Duplicate rows: 164087\n",
            "Label distribution:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Label distribution:')\n",
        "print(df['Label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3DUcIp74-Rj",
        "outputId": "5f147ff0-8f48-445f-9f0b-db52e037506f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label distribution:\n",
            "Label\n",
            "Anomaly    585710\n",
            "Normal      40073\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning\n"
      ],
      "metadata": {
        "id": "ZsfsN69E5UJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop missing values (if any)\n",
        "df_clean = df.dropna()\n",
        "# Normalize column names\n",
        "df_clean.columns = (\n",
        "    df_clean.columns\n",
        "      .str.strip()\n",
        "      .str.lower()\n",
        "      .str.replace(' ', '_')\n",
        ")\n",
        "print(f'After cleaning: {df_clean.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PPKZEak5I2D",
        "outputId": "3d3f08c6-fcb1-4879-db6e-fd8ec5374c7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After cleaning: (625783, 86)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Y4PDvLL8GDt",
        "outputId": "6e6ae9fa-01c1-4aef-855d-25a2d22bccbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['flow_id', 'src_ip', 'src_port', 'dst_ip', 'dst_port', 'protocol',\n",
              "       'timestamp', 'flow_duration', 'tot_fwd_pkts', 'tot_bwd_pkts',\n",
              "       'totlen_fwd_pkts', 'totlen_bwd_pkts', 'fwd_pkt_len_max',\n",
              "       'fwd_pkt_len_min', 'fwd_pkt_len_mean', 'fwd_pkt_len_std',\n",
              "       'bwd_pkt_len_max', 'bwd_pkt_len_min', 'bwd_pkt_len_mean',\n",
              "       'bwd_pkt_len_std', 'flow_byts/s', 'flow_pkts/s', 'flow_iat_mean',\n",
              "       'flow_iat_std', 'flow_iat_max', 'flow_iat_min', 'fwd_iat_tot',\n",
              "       'fwd_iat_mean', 'fwd_iat_std', 'fwd_iat_max', 'fwd_iat_min',\n",
              "       'bwd_iat_tot', 'bwd_iat_mean', 'bwd_iat_std', 'bwd_iat_max',\n",
              "       'bwd_iat_min', 'fwd_psh_flags', 'bwd_psh_flags', 'fwd_urg_flags',\n",
              "       'bwd_urg_flags', 'fwd_header_len', 'bwd_header_len', 'fwd_pkts/s',\n",
              "       'bwd_pkts/s', 'pkt_len_min', 'pkt_len_max', 'pkt_len_mean',\n",
              "       'pkt_len_std', 'pkt_len_var', 'fin_flag_cnt', 'syn_flag_cnt',\n",
              "       'rst_flag_cnt', 'psh_flag_cnt', 'ack_flag_cnt', 'urg_flag_cnt',\n",
              "       'cwe_flag_count', 'ece_flag_cnt', 'down/up_ratio', 'pkt_size_avg',\n",
              "       'fwd_seg_size_avg', 'bwd_seg_size_avg', 'fwd_byts/b_avg',\n",
              "       'fwd_pkts/b_avg', 'fwd_blk_rate_avg', 'bwd_byts/b_avg',\n",
              "       'bwd_pkts/b_avg', 'bwd_blk_rate_avg', 'subflow_fwd_pkts',\n",
              "       'subflow_fwd_byts', 'subflow_bwd_pkts', 'subflow_bwd_byts',\n",
              "       'init_fwd_win_byts', 'init_bwd_win_byts', 'fwd_act_data_pkts',\n",
              "       'fwd_seg_size_min', 'active_mean', 'active_std', 'active_max',\n",
              "       'active_min', 'idle_mean', 'idle_std', 'idle_max', 'idle_min', 'label',\n",
              "       'cat', 'sub_cat'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove invalid flows by filtering non-positive values\n",
        "df_clean = df_clean[df_clean['flow_duration'] > 0]\n",
        "df_clean = df_clean[df_clean['tot_fwd_pkts'] > 0]\n",
        "\n",
        "print(\"Columns after filtering:\", df_clean.columns.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkCoz2ov7pq3",
        "outputId": "83e7ddaa-0eb9-4576-cf72-8971ad3c25a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns after filtering: ['flow_id', 'src_ip', 'src_port', 'dst_ip', 'dst_port', 'protocol', 'timestamp', 'flow_duration', 'tot_fwd_pkts', 'tot_bwd_pkts', 'totlen_fwd_pkts', 'totlen_bwd_pkts', 'fwd_pkt_len_max', 'fwd_pkt_len_min', 'fwd_pkt_len_mean', 'fwd_pkt_len_std', 'bwd_pkt_len_max', 'bwd_pkt_len_min', 'bwd_pkt_len_mean', 'bwd_pkt_len_std', 'flow_byts/s', 'flow_pkts/s', 'flow_iat_mean', 'flow_iat_std', 'flow_iat_max', 'flow_iat_min', 'fwd_iat_tot', 'fwd_iat_mean', 'fwd_iat_std', 'fwd_iat_max', 'fwd_iat_min', 'bwd_iat_tot', 'bwd_iat_mean', 'bwd_iat_std', 'bwd_iat_max', 'bwd_iat_min', 'fwd_psh_flags', 'bwd_psh_flags', 'fwd_urg_flags', 'bwd_urg_flags', 'fwd_header_len', 'bwd_header_len', 'fwd_pkts/s', 'bwd_pkts/s', 'pkt_len_min', 'pkt_len_max', 'pkt_len_mean', 'pkt_len_std', 'pkt_len_var', 'fin_flag_cnt', 'syn_flag_cnt', 'rst_flag_cnt', 'psh_flag_cnt', 'ack_flag_cnt', 'urg_flag_cnt', 'cwe_flag_count', 'ece_flag_cnt', 'down/up_ratio', 'pkt_size_avg', 'fwd_seg_size_avg', 'bwd_seg_size_avg', 'fwd_byts/b_avg', 'fwd_pkts/b_avg', 'fwd_blk_rate_avg', 'bwd_byts/b_avg', 'bwd_pkts/b_avg', 'bwd_blk_rate_avg', 'subflow_fwd_pkts', 'subflow_fwd_byts', 'subflow_bwd_pkts', 'subflow_bwd_byts', 'init_fwd_win_byts', 'init_bwd_win_byts', 'fwd_act_data_pkts', 'fwd_seg_size_min', 'active_mean', 'active_std', 'active_max', 'active_min', 'idle_mean', 'idle_std', 'idle_max', 'idle_min', 'label', 'cat', 'sub_cat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clip extreme values at 1st and 99th percentiles\n",
        "for col in ['flow_duration', 'totlen_fwd_pkts', 'totlen_bwd_pkts']:\n",
        "    low, high = df_clean[col].quantile([0.01, 0.99])\n",
        "    df_clean[col] = df_clean[col].clip(lower=low, upper=high)"
      ],
      "metadata": {
        "id": "tBPaDgId9OOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering & Preprocessing\n"
      ],
      "metadata": {
        "id": "03mSeRjG5a-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols = [\n",
        "    'flow_duration',\n",
        "    'totlen_fwd_pkts',    # or 'total_length'\n",
        "    'totlen_bwd_pkts',    # or drop if using total_length\n",
        "    'src_port',\n",
        "    'dst_port'\n",
        "]\n"
      ],
      "metadata": {
        "id": "oqDlACmGJGdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "df_clean['label_enc'] = le.fit_transform(df_clean['label'])\n",
        "\n",
        "# Select X and y\n",
        "X = df_clean[feature_cols]\n",
        "y = df_clean['label_enc']\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "print('Train:', X_train.shape, 'Test:', X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnoCvItZ5X4w",
        "outputId": "1aa5ca3c-a547-4d3f-e469-0375db6c7a06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (321466, 5) Test: (80367, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaling & Saving Artifacts\n"
      ],
      "metadata": {
        "id": "CPpZzBIw5iYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "data_path = '/content/drive/MyDrive/Datasets/IoT Network Intrusion Dataset.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# derive artifact_dir from the data file’s parent folder:\n",
        "base_dir     = os.path.dirname(data_path)              # '/content/drive/MyDrive/Datasets'\n",
        "artifact_dir = os.path.join(base_dir, 'artifacts')     # '/content/drive/MyDrive/Datasets/artifacts'\n",
        "os.makedirs(artifact_dir, exist_ok=True)\n",
        "\n",
        "print(\"Artifacts will be saved to:\", artifact_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QQQ7sTBLZwh",
        "outputId": "c1e4f9b5-f85f-496d-ca83-8ee446304bbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artifacts will be saved to: /content/drive/MyDrive/Datasets/artifacts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "# 2. Persist scaler and label encoder\n",
        "artifact_dir = '/content/drive/MyDrive/Datasets/artifacts'\n",
        "os.makedirs(artifact_dir, exist_ok=True)\n",
        "joblib.dump(scaler, os.path.join(artifact_dir, 'scaler.joblib'))\n",
        "joblib.dump(le,     os.path.join(artifact_dir, 'label_encoder.joblib'))\n",
        "\n",
        "# 3. Save processed datasets\n",
        "#   Note: feature_cols must match the columns used to build X_train/X_test!\n",
        "pd.DataFrame(\n",
        "    X_train_scaled,\n",
        "    columns=[\n",
        "        'flow_duration',\n",
        "        'totlen_fwd_pkts',\n",
        "        'totlen_bwd_pkts',\n",
        "        'src_port',\n",
        "        'dst_port'\n",
        "    ]\n",
        ").to_csv(os.path.join(artifact_dir, 'X_train.csv'), index=False)\n",
        "\n",
        "pd.Series(y_train, name='label').to_csv(\n",
        "    os.path.join(artifact_dir, 'y_train.csv'),\n",
        "    index=False\n",
        ")\n",
        "\n",
        "pd.DataFrame(\n",
        "    X_test_scaled,\n",
        "    columns=[\n",
        "        'flow_duration',\n",
        "        'totlen_fwd_pkts',\n",
        "        'totlen_bwd_pkts',\n",
        "        'src_port',\n",
        "        'dst_port'\n",
        "    ]\n",
        ").to_csv(os.path.join(artifact_dir, 'X_test.csv'), index=False)\n",
        "\n",
        "pd.Series(y_test, name='label').to_csv(\n",
        "    os.path.join(artifact_dir, 'y_test.csv'),\n",
        "    index=False\n",
        ")\n",
        "\n",
        "print('Artifacts saved to', artifact_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzEECqSY5gXk",
        "outputId": "55264473-84e4-4257-fa3d-1461e95ced61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artifacts saved to /content/drive/MyDrive/Datasets/artifacts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JznfcJCrMiUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib, pandas as pd\n",
        "\n",
        "X_train = pd.read_csv(f\"{artifact_dir}/X_train.csv\")\n",
        "y_train = pd.read_csv(f\"{artifact_dir}/y_train.csv\")['label']\n"
      ],
      "metadata": {
        "id": "63Ll2iIC5mjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate and fit:"
      ],
      "metadata": {
        "id": "4bF6NmqOMsMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "model = IsolationForest(contamination=0.01, random_state=42)\n",
        "model.fit(X_train)\n",
        "joblib.dump(model, f\"{artifact_dir}/model.joblib\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cN1MA3pcMjEF",
        "outputId": "7f6e79e9-7fcd-48dc-84f9-cdb7f20f73b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Datasets/artifacts/model.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = pd.read_csv(f\"{artifact_dir}/X_test.csv\")\n",
        "y_test = pd.read_csv(f\"{artifact_dir}/y_test.csv\")['label']\n",
        "y_pred = model.predict(X_test)           # [-1, 1] for IsolationForest\n",
        "# or clf.predict(X_test) for DecisionTree\n"
      ],
      "metadata": {
        "id": "tznSmx-IMuk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ],
      "metadata": {
        "id": "UDA7rg7uN29Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Cell 1: Mount Drive and Imports\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBSfj5WeMyQ-",
        "outputId": "020acea3-2501-4bc9-9bf3-986ea41d0e60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Cell 2: Load Preprocessing & Model Artifacts\n",
        "artifact_dir = '/content/drive/MyDrive/Datasets/artifacts'  # adjust if needed\n",
        "\n",
        "# These should match what you saved earlier\n",
        "scaler = joblib.load(f\"{artifact_dir}/scaler.joblib\")\n",
        "le     = joblib.load(f\"{artifact_dir}/label_encoder.joblib\")\n",
        "model  = joblib.load(f\"{artifact_dir}/model.joblib\")       # IsolationForest or classifier\n"
      ],
      "metadata": {
        "id": "90_ke08GN5GN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Cell 3: Simulate 2–3 “Realistic” Flow Samples\n",
        "# Make sure your keys match feature_cols exactly\n",
        "feature_cols = ['flow_duration', 'totlen_fwd_pkts', 'totlen_bwd_pkts', 'src_port', 'dst_port']\n",
        "\n",
        "samples = [\n",
        "    {'flow_duration': 1200, 'totlen_fwd_pkts': 1500, 'totlen_bwd_pkts': 1400, 'src_port': 34567, 'dst_port': 80},\n",
        "    {'flow_duration': 50,   'totlen_fwd_pkts': 30,   'totlen_bwd_pkts': 25,   'src_port': 5678,  'dst_port': 443},\n",
        "    {'flow_duration': 300,  'totlen_fwd_pkts': 300,  'totlen_bwd_pkts': 310,  'src_port': 12345, 'dst_port': 22}\n",
        "]\n",
        "\n",
        "df_samples = pd.DataFrame(samples, columns=feature_cols)\n",
        "print(\"Raw samples:\\n\", df_samples)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mU3K8M9EN7-Q",
        "outputId": "6ec68d87-bfce-49f9-e485-35c7deecda47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw samples:\n",
            "    flow_duration  totlen_fwd_pkts  totlen_bwd_pkts  src_port  dst_port\n",
            "0           1200             1500             1400     34567        80\n",
            "1             50               30               25      5678       443\n",
            "2            300              300              310     12345        22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Cell 4: Scale & Predict\n",
        "X_sim_scaled = scaler.transform(df_samples)\n",
        "\n",
        "# If using IsolationForest:\n",
        "if hasattr(model, \"decision_function\"):\n",
        "    scores   = model.decision_function(X_sim_scaled)      # higher = more “normal”\n",
        "    preds    = model.predict(X_sim_scaled)                # 1 = normal, -1 = anomaly\n",
        "    df_results = df_samples.copy()\n",
        "    df_results['score']   = scores\n",
        "    df_results['anomaly'] = preds == -1\n",
        "else:\n",
        "    # For a supervised classifier\n",
        "    raw_preds   = model.predict(X_sim_scaled)\n",
        "    # decode back to original labels\n",
        "    decoded     = le.inverse_transform(raw_preds)\n",
        "    df_results = df_samples.copy()\n",
        "    df_results['predicted_label'] = decoded\n",
        "\n",
        "print(\"\\nResults:\\n\", df_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxCtvyzvN-P5",
        "outputId": "bd10aebd-66df-4a32-f2ed-97e02c937778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results:\n",
            "    flow_duration  totlen_fwd_pkts  totlen_bwd_pkts  src_port  dst_port  \\\n",
            "0           1200             1500             1400     34567        80   \n",
            "1             50               30               25      5678       443   \n",
            "2            300              300              310     12345        22   \n",
            "\n",
            "      score  anomaly  \n",
            "0 -0.019066     True  \n",
            "1  0.116947    False  \n",
            "2  0.058294    False  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Cell: Readable Anomaly Detection Output (Fixed)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "\n",
        "# 1. Compute normalized scores (0–1) from raw decision_function outputs\n",
        "raw_scores = model.decision_function(X_sim_scaled)\n",
        "min_s, max_s = raw_scores.min(), raw_scores.max()\n",
        "norm_scores = (raw_scores - min_s) / (max_s - min_s)\n",
        "\n",
        "# 2. Map to human-readable labels\n",
        "labels = ['Normal' if s >= 0.5 else 'Anomalous' for s in norm_scores]\n",
        "\n",
        "# 3. Create simple explanations for each sample\n",
        "reasons = []\n",
        "for idx, row in df_samples.iterrows():\n",
        "    if labels[idx] == 'Anomalous':\n",
        "        reasons.append('Duration or packet counts outside normal range')\n",
        "    else:\n",
        "        reasons.append('Within expected network behavior')\n",
        "\n",
        "# 4. Assemble display DataFrame\n",
        "df_display = df_samples.copy()\n",
        "df_display['AnomalyScore'] = [f\"{s:.1%}\" for s in norm_scores]\n",
        "df_display['Label']        = labels\n",
        "df_display['Reason']       = reasons\n",
        "\n",
        "# 5. Style and display the table, highlighting anomalies\n",
        "def highlight_anomaly(row):\n",
        "    return ['background-color: #faa' if row.Label=='Anomalous' else '' for _ in row]\n",
        "\n",
        "styled = (\n",
        "    df_display.style\n",
        "      .apply(highlight_anomaly, axis=1)\n",
        "      .set_caption(\"Anomaly Detection Results\")\n",
        "      .hide(axis=\"index\")                # ← replaces .hide_index()\n",
        ")\n",
        "\n",
        "display(styled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "BEvwnWXaOBQK",
        "outputId": "65650286-c098-4e57-bdf8-5bb8ecb8a344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7e802c503910>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_19990_row0_col0, #T_19990_row0_col1, #T_19990_row0_col2, #T_19990_row0_col3, #T_19990_row0_col4, #T_19990_row0_col5, #T_19990_row0_col6, #T_19990_row0_col7 {\n",
              "  background-color: #faa;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_19990\" class=\"dataframe\">\n",
              "  <caption>Anomaly Detection Results</caption>\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_19990_level0_col0\" class=\"col_heading level0 col0\" >flow_duration</th>\n",
              "      <th id=\"T_19990_level0_col1\" class=\"col_heading level0 col1\" >totlen_fwd_pkts</th>\n",
              "      <th id=\"T_19990_level0_col2\" class=\"col_heading level0 col2\" >totlen_bwd_pkts</th>\n",
              "      <th id=\"T_19990_level0_col3\" class=\"col_heading level0 col3\" >src_port</th>\n",
              "      <th id=\"T_19990_level0_col4\" class=\"col_heading level0 col4\" >dst_port</th>\n",
              "      <th id=\"T_19990_level0_col5\" class=\"col_heading level0 col5\" >AnomalyScore</th>\n",
              "      <th id=\"T_19990_level0_col6\" class=\"col_heading level0 col6\" >Label</th>\n",
              "      <th id=\"T_19990_level0_col7\" class=\"col_heading level0 col7\" >Reason</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_19990_row0_col0\" class=\"data row0 col0\" >1200</td>\n",
              "      <td id=\"T_19990_row0_col1\" class=\"data row0 col1\" >1500</td>\n",
              "      <td id=\"T_19990_row0_col2\" class=\"data row0 col2\" >1400</td>\n",
              "      <td id=\"T_19990_row0_col3\" class=\"data row0 col3\" >34567</td>\n",
              "      <td id=\"T_19990_row0_col4\" class=\"data row0 col4\" >80</td>\n",
              "      <td id=\"T_19990_row0_col5\" class=\"data row0 col5\" >0.0%</td>\n",
              "      <td id=\"T_19990_row0_col6\" class=\"data row0 col6\" >Anomalous</td>\n",
              "      <td id=\"T_19990_row0_col7\" class=\"data row0 col7\" >Duration or packet counts outside normal range</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_19990_row1_col0\" class=\"data row1 col0\" >50</td>\n",
              "      <td id=\"T_19990_row1_col1\" class=\"data row1 col1\" >30</td>\n",
              "      <td id=\"T_19990_row1_col2\" class=\"data row1 col2\" >25</td>\n",
              "      <td id=\"T_19990_row1_col3\" class=\"data row1 col3\" >5678</td>\n",
              "      <td id=\"T_19990_row1_col4\" class=\"data row1 col4\" >443</td>\n",
              "      <td id=\"T_19990_row1_col5\" class=\"data row1 col5\" >100.0%</td>\n",
              "      <td id=\"T_19990_row1_col6\" class=\"data row1 col6\" >Normal</td>\n",
              "      <td id=\"T_19990_row1_col7\" class=\"data row1 col7\" >Within expected network behavior</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_19990_row2_col0\" class=\"data row2 col0\" >300</td>\n",
              "      <td id=\"T_19990_row2_col1\" class=\"data row2 col1\" >300</td>\n",
              "      <td id=\"T_19990_row2_col2\" class=\"data row2 col2\" >310</td>\n",
              "      <td id=\"T_19990_row2_col3\" class=\"data row2 col3\" >12345</td>\n",
              "      <td id=\"T_19990_row2_col4\" class=\"data row2 col4\" >22</td>\n",
              "      <td id=\"T_19990_row2_col5\" class=\"data row2 col5\" >56.9%</td>\n",
              "      <td id=\"T_19990_row2_col6\" class=\"data row2 col6\" >Normal</td>\n",
              "      <td id=\"T_19990_row2_col7\" class=\"data row2 col7\" >Within expected network behavior</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More Tests"
      ],
      "metadata": {
        "id": "O0ZcSlsyPk0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Cell: Readable Anomaly Detection Output with Multiple Examples\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "\n",
        "# -- 1. Define 5 sample flows covering different scenarios --\n",
        "feature_cols = ['flow_duration', 'totlen_fwd_pkts', 'totlen_bwd_pkts', 'src_port', 'dst_port']\n",
        "sample_data = [\n",
        "    # 0. Typical Web Browsing (normal)\n",
        "    {'flow_duration': 100,  'totlen_fwd_pkts': 80,  'totlen_bwd_pkts': 75,  'src_port': 49152, 'dst_port': 80},\n",
        "    # 1. Small IoT Heartbeat (normal, low traffic)\n",
        "    {'flow_duration': 20,   'totlen_fwd_pkts': 5,   'totlen_bwd_pkts': 5,   'src_port': 37000, 'dst_port': 5683},\n",
        "    # 2. Borderline: longer than usual, moderate packets\n",
        "    {'flow_duration': 800,  'totlen_fwd_pkts': 200, 'totlen_bwd_pkts': 195, 'src_port': 40000, 'dst_port': 443},\n",
        "    # 3. High-volume anomaly: extremely large packet counts\n",
        "    {'flow_duration': 1500, 'totlen_fwd_pkts': 5000,'totlen_bwd_pkts': 4800,'src_port': 34567, 'dst_port': 22},\n",
        "    # 4. Very short but many packets (possible flooding)\n",
        "    {'flow_duration': 5,    'totlen_fwd_pkts': 1000,'totlen_bwd_pkts': 950, 'src_port': 12345, 'dst_port': 8080},\n",
        "]\n",
        "df_samples = pd.DataFrame(sample_data, columns=feature_cols)\n",
        "\n",
        "# -- 2. Scale samples --\n",
        "X_sim_scaled = scaler.transform(df_samples)\n",
        "\n",
        "# -- 3. Compute and normalize raw anomaly scores --\n",
        "raw_scores = model.decision_function(X_sim_scaled)\n",
        "min_s, max_s = raw_scores.min(), raw_scores.max()\n",
        "norm_scores = (raw_scores - min_s) / (max_s - min_s)\n",
        "\n",
        "# -- 4. Map to human-readable labels --\n",
        "labels = ['Normal' if s >= 0.5 else 'Anomalous' for s in norm_scores]\n",
        "\n",
        "# -- 5. Generate explanations based on simple thresholds --\n",
        "reasons = []\n",
        "for i, row in df_samples.iterrows():\n",
        "    if labels[i] == 'Anomalous':\n",
        "        # choose reason based on which feature is extreme\n",
        "        if row['totlen_fwd_pkts'] > 1000:\n",
        "            reasons.append('High packet count → potential flood')\n",
        "        elif row['flow_duration'] < 10 and row['totlen_fwd_pkts'] > 500:\n",
        "            reasons.append('Very short, high-volume → likely scan/flood')\n",
        "        else:\n",
        "            reasons.append('Unusually long or voluminous flow')\n",
        "    else:\n",
        "        reasons.append('Within expected IoT/network behavior')\n",
        "\n",
        "# -- 6. Assemble display DataFrame --\n",
        "df_display = df_samples.copy()\n",
        "df_display['AnomalyScore'] = [f\"{s:.1%}\" for s in norm_scores]\n",
        "df_display['Label']        = labels\n",
        "df_display['Reason']       = reasons\n",
        "\n",
        "# -- 7. Style & display --\n",
        "def highlight_anomaly(row):\n",
        "    return ['background-color: #faa' if row.Label == 'Anomalous' else '' for _ in row]\n",
        "\n",
        "styled = (\n",
        "    df_display.style\n",
        "      .apply(highlight_anomaly, axis=1)\n",
        "      .set_caption(\"Anomaly Detection Results: Sample Scenarios\")\n",
        "      .hide(axis=\"index\")\n",
        ")\n",
        "\n",
        "display(styled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "FnQDunAXPDYw",
        "outputId": "5580b16d-e789-4cc9-d4da-37035d444efe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7e802ceb5d90>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_1d310_row2_col0, #T_1d310_row2_col1, #T_1d310_row2_col2, #T_1d310_row2_col3, #T_1d310_row2_col4, #T_1d310_row2_col5, #T_1d310_row2_col6, #T_1d310_row2_col7, #T_1d310_row3_col0, #T_1d310_row3_col1, #T_1d310_row3_col2, #T_1d310_row3_col3, #T_1d310_row3_col4, #T_1d310_row3_col5, #T_1d310_row3_col6, #T_1d310_row3_col7 {\n",
              "  background-color: #faa;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_1d310\" class=\"dataframe\">\n",
              "  <caption>Anomaly Detection Results: Sample Scenarios</caption>\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_1d310_level0_col0\" class=\"col_heading level0 col0\" >flow_duration</th>\n",
              "      <th id=\"T_1d310_level0_col1\" class=\"col_heading level0 col1\" >totlen_fwd_pkts</th>\n",
              "      <th id=\"T_1d310_level0_col2\" class=\"col_heading level0 col2\" >totlen_bwd_pkts</th>\n",
              "      <th id=\"T_1d310_level0_col3\" class=\"col_heading level0 col3\" >src_port</th>\n",
              "      <th id=\"T_1d310_level0_col4\" class=\"col_heading level0 col4\" >dst_port</th>\n",
              "      <th id=\"T_1d310_level0_col5\" class=\"col_heading level0 col5\" >AnomalyScore</th>\n",
              "      <th id=\"T_1d310_level0_col6\" class=\"col_heading level0 col6\" >Label</th>\n",
              "      <th id=\"T_1d310_level0_col7\" class=\"col_heading level0 col7\" >Reason</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_1d310_row0_col0\" class=\"data row0 col0\" >100</td>\n",
              "      <td id=\"T_1d310_row0_col1\" class=\"data row0 col1\" >80</td>\n",
              "      <td id=\"T_1d310_row0_col2\" class=\"data row0 col2\" >75</td>\n",
              "      <td id=\"T_1d310_row0_col3\" class=\"data row0 col3\" >49152</td>\n",
              "      <td id=\"T_1d310_row0_col4\" class=\"data row0 col4\" >80</td>\n",
              "      <td id=\"T_1d310_row0_col5\" class=\"data row0 col5\" >81.2%</td>\n",
              "      <td id=\"T_1d310_row0_col6\" class=\"data row0 col6\" >Normal</td>\n",
              "      <td id=\"T_1d310_row0_col7\" class=\"data row0 col7\" >Within expected IoT/network behavior</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_1d310_row1_col0\" class=\"data row1 col0\" >20</td>\n",
              "      <td id=\"T_1d310_row1_col1\" class=\"data row1 col1\" >5</td>\n",
              "      <td id=\"T_1d310_row1_col2\" class=\"data row1 col2\" >5</td>\n",
              "      <td id=\"T_1d310_row1_col3\" class=\"data row1 col3\" >37000</td>\n",
              "      <td id=\"T_1d310_row1_col4\" class=\"data row1 col4\" >5683</td>\n",
              "      <td id=\"T_1d310_row1_col5\" class=\"data row1 col5\" >100.0%</td>\n",
              "      <td id=\"T_1d310_row1_col6\" class=\"data row1 col6\" >Normal</td>\n",
              "      <td id=\"T_1d310_row1_col7\" class=\"data row1 col7\" >Within expected IoT/network behavior</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_1d310_row2_col0\" class=\"data row2 col0\" >800</td>\n",
              "      <td id=\"T_1d310_row2_col1\" class=\"data row2 col1\" >200</td>\n",
              "      <td id=\"T_1d310_row2_col2\" class=\"data row2 col2\" >195</td>\n",
              "      <td id=\"T_1d310_row2_col3\" class=\"data row2 col3\" >40000</td>\n",
              "      <td id=\"T_1d310_row2_col4\" class=\"data row2 col4\" >443</td>\n",
              "      <td id=\"T_1d310_row2_col5\" class=\"data row2 col5\" >31.4%</td>\n",
              "      <td id=\"T_1d310_row2_col6\" class=\"data row2 col6\" >Anomalous</td>\n",
              "      <td id=\"T_1d310_row2_col7\" class=\"data row2 col7\" >Unusually long or voluminous flow</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_1d310_row3_col0\" class=\"data row3 col0\" >1500</td>\n",
              "      <td id=\"T_1d310_row3_col1\" class=\"data row3 col1\" >5000</td>\n",
              "      <td id=\"T_1d310_row3_col2\" class=\"data row3 col2\" >4800</td>\n",
              "      <td id=\"T_1d310_row3_col3\" class=\"data row3 col3\" >34567</td>\n",
              "      <td id=\"T_1d310_row3_col4\" class=\"data row3 col4\" >22</td>\n",
              "      <td id=\"T_1d310_row3_col5\" class=\"data row3 col5\" >0.0%</td>\n",
              "      <td id=\"T_1d310_row3_col6\" class=\"data row3 col6\" >Anomalous</td>\n",
              "      <td id=\"T_1d310_row3_col7\" class=\"data row3 col7\" >High packet count → potential flood</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_1d310_row4_col0\" class=\"data row4 col0\" >5</td>\n",
              "      <td id=\"T_1d310_row4_col1\" class=\"data row4 col1\" >1000</td>\n",
              "      <td id=\"T_1d310_row4_col2\" class=\"data row4 col2\" >950</td>\n",
              "      <td id=\"T_1d310_row4_col3\" class=\"data row4 col3\" >12345</td>\n",
              "      <td id=\"T_1d310_row4_col4\" class=\"data row4 col4\" >8080</td>\n",
              "      <td id=\"T_1d310_row4_col5\" class=\"data row4 col5\" >53.9%</td>\n",
              "      <td id=\"T_1d310_row4_col6\" class=\"data row4 col6\" >Normal</td>\n",
              "      <td id=\"T_1d310_row4_col7\" class=\"data row4 col7\" >Within expected IoT/network behavior</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: find the section for  \"Data Preprocessing Code\"\n",
        "\n",
        "# ## Feature Engineering & Preprocessing\n",
        "feature_cols = [\n",
        "    'flow_duration',\n",
        "    'totlen_fwd_pkts',    # or 'total_length'\n",
        "    'totlen_bwd_pkts',    # or drop if using total_length\n",
        "    'src_port',\n",
        "    'dst_port'\n",
        "]\n",
        "\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "df_clean['label_enc'] = le.fit_transform(df_clean['label'])\n",
        "\n",
        "# Select X and y\n",
        "X = df_clean[feature_cols]\n",
        "y = df_clean['label_enc']\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "print('Train:', X_train.shape, 'Test:', X_test.shape)\n"
      ],
      "metadata": {
        "id": "AiR4RVaKly_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: ML Model Training Code\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Train a supervised classification model (e.g., Logistic Regression)\n",
        "# Note: This is an example. You might use other classifiers like RandomForest, GradientBoosting, etc.\n",
        "classifier_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "classifier_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_classifier = classifier_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the classifier model\n",
        "print(\"\\nClassifier Model Evaluation:\")\n",
        "print(confusion_matrix(y_test, y_pred_classifier))\n",
        "print(classification_report(y_test, y_pred_classifier, target_names=le.classes_))\n",
        "\n",
        "# Save the trained classifier model\n",
        "joblib.dump(classifier_model, os.path.join(artifact_dir, 'classifier_model.joblib'))\n",
        "print('Classifier model saved to', os.path.join(artifact_dir, 'classifier_model.joblib'))\n"
      ],
      "metadata": {
        "id": "BRCAnnHfnTvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Explain the above project, what kind of project, which algorithm used ofr traning?... more\n",
        "\n",
        "This Colab notebook project is an anomaly detection system designed for IoT network intrusion detection using the IoTID20 dataset.\n",
        "\n",
        "**Project Type:** Anomaly Detection\n",
        "\n",
        "**Algorithm Used for Training:** The primary algorithm used for training the anomaly detection model is **Isolation Forest**.\n",
        "\n",
        "**How the Project Works:**\n",
        "\n",
        "1.  **Data Loading:** It starts by mounting Google Drive and loading the IoTID20 dataset, which presumably contains network flow data with labels indicating normal or malicious activity.\n",
        "2.  **Data Cleaning and Preprocessing:**\n",
        "    *   It performs initial inspection of the data types, missing values, duplicates, and label distribution.\n",
        "    *   Missing values are dropped.\n",
        "    *   Column names are cleaned and standardized.\n",
        "    *   Invalid flows (with non-positive duration or forward packet counts) are removed.\n",
        "    *   Extreme values in certain numerical columns (`flow_duration`, `totlen_fwd_pkts`, `totlen_bwd_pkts`) are clipped to the 1st and 99th percentiles to handle outliers.\n",
        "3.  **Feature Engineering and Preprocessing:**\n",
        "    *   A subset of features (`flow_duration`, `totlen_fwd_pkts`, `totlen_bwd_pkts`, `src_port`, `dst_port`) is selected for the model.\n",
        "    *   The 'Label' column (presumably indicating normal/anomaly) is encoded into numerical values using `LabelEncoder`.\n",
        "    *   The data is split into training and testing sets (`X_train`, `X_test`, `y_train`, `y_test`) using `train_test_split`, ensuring the label distribution is maintained (stratified split).\n",
        "4.  **Scaling and Artifact Saving:**\n",
        "    *   The selected features (`X_train`, `X_test`) are scaled using `StandardScaler` to standardize their ranges.\n",
        "    *   The fitted `scaler` and `label_encoder` objects are saved to disk using `joblib` as artifacts.\n",
        "    *   The processed, scaled training and testing datasets (`X_train.csv`, `y_train.csv`, `X_test.csv`, `y_test.csv`) are saved as CSV files.\n",
        "5.  **Model Training:**\n",
        "    *   An `IsolationForest` model is instantiated. This is an unsupervised learning algorithm specifically designed for anomaly detection. It works by isolating observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. Anomalies are expected to be isolated in fewer splitting steps than normal instances.\n",
        "    *   The model is trained (`fit`) on the scaled training features (`X_train`).\n",
        "    *   The trained `IsolationForest` model is saved to disk using `joblib`.\n",
        "6.  **Testing and Prediction:**\n",
        "    *   The processed test data (`X_test`, `y_test`) is loaded.\n",
        "    *   The trained model is used to make predictions (`predict`) on the scaled test features (`X_test`). Isolation Forest predicts `1` for normal instances and `-1` for anomalies.\n",
        "7.  **Simulating and Evaluating New Samples:**\n",
        "    *   The notebook includes sections to simulate new, realistic network flow samples.\n",
        "    *   These new samples are scaled using the previously saved `scaler`.\n",
        "    *   The trained model is used to predict anomaly scores (`decision_function`) or labels (`predict`) for these samples.\n",
        "    *   The anomaly scores are normalized (0-1) for better interpretability.\n",
        "    *   Human-readable labels ('Normal' or 'Anomalous') are assigned based on a threshold (0.5 on the normalized score).\n",
        "    *   Simple explanations for why a sample might be flagged as anomalous are generated based on thresholding certain features (like high packet count or very short duration with high volume).\n",
        "    *   The results, including the raw sample data, normalized anomaly score, label, and reason, are displayed in a styled pandas DataFrame, with anomalous rows highlighted.\n",
        "\n",
        "**In summary,** the project focuses on building and testing an anomaly detection system for network traffic using the Isolation Forest algorithm. It covers the essential steps of data preprocessing, model training, artifact saving, and evaluating the model's ability to detect anomalies on new, unseen data. The Isolation Forest is chosen for its effectiveness in identifying outliers (anomalies) in high-dimensional datasets."
      ],
      "metadata": {
        "id": "UBiZhdqkPnTS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}